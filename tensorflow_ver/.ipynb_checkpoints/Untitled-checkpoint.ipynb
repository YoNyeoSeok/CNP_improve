{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.linalg import cholesky\n",
    "\n",
    "class Net_h():\n",
    "    def __init__(self, dim_input, dim_hiddens):\n",
    "        self.dim_input = dim_input \n",
    "        self.dim_hiddens = dim_hiddens\n",
    "\n",
    "    ## trian, test flow    \n",
    "    def construct_model(self, O):\n",
    "         \n",
    "        pass\n",
    "\n",
    "    ## initial weights\n",
    "    def construct_weights(self):\n",
    "        weights = {}\n",
    "        weights['w1'] = tf.Variable(tf.truncated_normal([self.dim_input, self.dim_hiddens[0]], stddev=0.01))\n",
    "        weights['b1'] = tf.Variable(tf.zeros(self.dim_hiddens[0]))\n",
    "        for i in range(1, len(self.dim_hiddens)):\n",
    "            weights['w'+str(i+1)] = tf.Variable(tf.truncated_normal([self.dim_hiddens[i-1], self.dim_hiddens[i]], stddev=0.01))\n",
    "            weights['b'+str(i+1)] = tf.Variable(tf.zeros(self.dim_hiddens[i]))\n",
    "        return weights\n",
    "\n",
    "    ## forward path\n",
    "    def forward(self, inp, weights):\n",
    "        outp = tf.matmul(inp, weights['w1']) + weights['b1']\n",
    "        acti = tf.nn.relu(outp)\n",
    "        for i in range(1, len(self.dim_hiddens)):\n",
    "            outp = tf.matmul(acti, weights['w'+str(i+1)]) + weights['b'+str(i+1)]\n",
    "            acti = tf.nn.relu(outp)\n",
    "        return acti\n",
    "\n",
    "class Net_g():\n",
    "    def __init__(self, dim_hiddens, dim_output):\n",
    "        self.dim_hiddens = dim_hiddens\n",
    "        self.dim_output = dim_output \n",
    "\n",
    "    ## trian, test flow    \n",
    "    def construct_model(self, O):\n",
    "         \n",
    "        pass\n",
    "\n",
    "    ## initial weights\n",
    "    def construct_weights(self):\n",
    "        weights = {}\n",
    "        for i in range(len(self.dim_hiddens)-1):\n",
    "            weights['w'+str(i)] = tf.Variable(tf.truncated_normal([self.dim_hiddens[i], self.dim_hiddens[i+1]], stddev=0.01))\n",
    "            weights['b'+str(i)] = tf.Variable(tf.zeros(self.dim_hiddens[i+1]))\n",
    "        weights['w'+str(len(self.dim_hiddens))] = tf.Variable(tf.truncated_normal([self.dim_hiddens[len(self.dim_hiddens)-1], self.dim_output], stddev=0.01))\n",
    "        weights['b'+str(len(self.dim_hiddens))] = tf.Variable(tf.zeros(self.dim_output))\n",
    "        return weights\n",
    "\n",
    "    ## forward path\n",
    "    def forward(self, inp, weights):\n",
    "        for i in range(len(self.dim_hiddens)):\n",
    "            outp = tf.matmul(acti, weights['w'+str(i)]) + weights['b'+str(i)]\n",
    "            acti = tf.nn.relu(outp)\n",
    "        outp = tf.matmul(inp, weights['w'+str(len(self.dim_hiddens))]) + weights['b'+str(len(self.dim_hiddens))]\n",
    "        acti = tf.nn.relu(outp)\n",
    "        return acti\n",
    "\n",
    "class CNP_Net():\n",
    "    def __init__(self, dim_io=[1,1], \\\n",
    "            dim_hiddens={'h':[8, 32, 128], 'g':[128, 64, 32, 16, 8]}):\n",
    "        self.dim_io = dim_io \n",
    "        self.dim_hiddens = dim_hiddens\n",
    "        self.net_h = Net_h(sum(dim_io), dim_hiddens['h'])\n",
    "        self.net_g = Net_g(dim_hiddens['g'], dim_io[1]**2)\n",
    "\n",
    "    def construct_model(self):\n",
    "        pass\n",
    "    \n",
    "    def construct_weights(self):\n",
    "        weights = {}\n",
    "        weights['g'] = self.net_g.construct_weights()\n",
    "        weights['h'] = self.net_h.construct_weights()\n",
    "\n",
    "#         weights_g = self.net_g.construct_weights()\n",
    "#         weights_h = self.net_h.construct_weights()\n",
    "#         weights = {}\n",
    "#         for keys_g, keys_h in zip(weights_g, weights_h):\n",
    "#             weights['g'+keys_g] = weights_g.pop(keys_g)\n",
    "#             weights['h'+keys_h] = weights_h.pop(keys_h)\n",
    "#         del weights_g, weights_h\n",
    "\n",
    "    def forward(self, O, T, weights_g, weights_h):\n",
    "        import time\n",
    "        t = time.time()\n",
    "        self.rs = self.net_h.forward(O, weights_h)\n",
    "        \"\"\"\n",
    "        self.r = self.operator(self.net_h(O), dim=0).expand(T.shape[0], -1)\n",
    "        self.xr = torch.cat((self.r, T[:,:self.io_dims[0]]), dim=1)\n",
    "        self.phi = self.net_g(self.xr)\n",
    "\n",
    "#        if self.io_dims[1] != 1: \n",
    "#            print(\"multivariate regression is not supported\")\n",
    "#            return\n",
    "        t = time.time()\n",
    "        self.mu = self.phi[:,:self.io_dims[1]]\n",
    "        self.sig = self.softplus(self.phi[:,self.io_dims[1]:])\n",
    "        \n",
    "        t = time.time()\n",
    "        log_probs = []\n",
    "        def func(m, s, t):\n",
    "            normal = MultivariateNormal(m, torch.diag(s**2))\n",
    "            return normal.log_prob(t)\n",
    "#        print(self.mu)\n",
    "#        print(self.sig)\n",
    "#        print(T)\n",
    "#        print(T[:, self.io_dims[0]:])\n",
    "        log_probs = list(map(func, self.mu, self.sig, T[:, self.io_dims[0]:]))\n",
    "#        for m, s, t in zip(self.mu, self.sig, T):\n",
    "#            #            normal = MultivariateNormal(m, torch.diag(s**2))\n",
    "##            log_probs.append(normal.log_prob(t[self.io_dims[0]:]))\n",
    "#            print(m)\n",
    "#            log_probs.append(t.log_normal_(m[0].cpu().numpy().tolist(), s[0].cpu().numpy().tolist()**2))\n",
    "#            log_probs.append(normal.log_prob(t[self.io_dims[0]:]))\n",
    "\n",
    "    \n",
    "#        t = time.time()\n",
    "#        normals = [MultivariateNormal(mu, torch.diag(cov)) for mu, cov in \n",
    "#                zip(self.mu, self.sig**2)]\n",
    "#        print('normals', time.time() - t)\n",
    "#        t = time.time()\n",
    "#        log_probs = [normals[i].log_prob(t[self.io_dims[0]:]) for i, t in enumerate(T)]\n",
    "#        print('log_probs', time.time() - t)\n",
    "\n",
    "        log_prob = 0\n",
    "        for p in log_probs:\n",
    "            log_prob += p\n",
    "        \n",
    "        return self.phi, log_prob/len(log_probs)\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNP_Net(dim_io = [1,1], dim_hiddens={'h':[8, 32, 128], 'g':[128, 64, 32, 16, 8]})\n",
    "\n",
    "model.construct_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python main.py \n",
    "\"\"\"\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# from model import CNP_Net\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--task_limit\", type=int, default=0,\n",
    "        help='number of task to train, 0 is infinite (default=0)')\n",
    "parser.add_argument(\"--max_epoch\", type=int, default=10000,\n",
    "        help='max iteration (default=10000')\n",
    "parser.add_argument(\"--interval\", type=int, default=1000,\n",
    "        help='max iteration (default=10000')\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=1e-4,\n",
    "        help=\"learning rate (default=1e-4)\")\n",
    "parser.add_argument(\"-bs\", \"--batch_size\", type=int, nargs='?', default=32,\n",
    "        help='batch size (default: 32)')\n",
    "parser.add_argument(\"-ns\", \"--num_samples\", type=int, nargs=2, default=[5, 5],\n",
    "        help='number of samples, few-shot number of train, test (default: [5, 5])')\n",
    "parser.add_argument(\"-nsr\", \"--num_samples_range\", metavar=('min', 'max'), type=int, nargs=2, default=[1, 51],\n",
    "        help='number of samples random range, few-shot number random range (default: [1 51])')\n",
    "parser.add_argument(\"-rs\", \"--random_sample\", action='store_true',\n",
    "        help='number of samples is random flag')\n",
    "parser.add_argument(\"-nnd\", \"--not_disjoint_data\", action='store_false',\n",
    "        help='train test set data are not disjoint setting flag')\n",
    "parser.add_argument(\"--input_range\", metavar=('min', 'max'), type=int, nargs=2, default=[-2, 2],\n",
    "        help='function input range (default: [-2, 2])')\n",
    "parser.add_argument(\"--output_range\", metavar=('min', 'max'), type=int, nargs=2, default=[-2, 2],\n",
    "        help='function output range (default: [-2, 2])')\n",
    "parser.add_argument(\"--log\", action='store_true',\n",
    "        help=\"save loss, fig and model log\")\n",
    "parser.add_argument(\"--log_folder\", type=str, default=\"log\",\n",
    "        help=\"log folder name in logs/ (default: log)\")\n",
    "parser.add_argument(\"--datasource\", type=str, nargs='?', default=\"gp1d\", choices=[\"gp1d\", \"branin\"],\n",
    "        help=\"gp1d or branin\")\n",
    "parser.add_argument(\"--fig_show\", action='store_true',\n",
    "        help=\"figure show during traing\")\n",
    "parser.add_argument(\"--gpu\", action='store_false',\n",
    "        help=\"use gpu\")\n",
    "parser.add_argument(\"--load_model\", type=str,\n",
    "        help=\"load model. format: folder/iteration\")\n",
    "parser.add_argument(\"--model_layers\", nargs='?', default=None,\n",
    "        help=\"model layers: default=None, means {'h':[8, 32, 128], 'g':[128, 64, 32, 16, 8]}\")\n",
    "parser.add_argument(\"--precision\", nargs='?', default=None,\n",
    "        help=\"change variance to precision (inverse) option\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    data_generator = DataGenerator(datasource=args.datasource, \n",
    "                                   batch_size=args.batch_size,\n",
    "                                   random_sample=args.random_sample,\n",
    "                                   disjoint_data=not args.not_disjoint_data,\n",
    "                                   num_samples=args.num_samples,\n",
    "                                   num_samples_range=args.num_samples_range,\n",
    "                                   input_range=args.input_range,\n",
    "                                   output_range=args.output_range,\n",
    "                                   task_limit=args.task_limit)\n",
    "    \n",
    "    space_samples = data_generator.generate_space_sample()\n",
    "\n",
    "    if args.load_model is not None:\n",
    "        model = torch.load(args.load_model)\n",
    "    elif args.model_layers is not None:\n",
    "        model = CNP_Net(io_dims=data_generator.io_dims, \n",
    "                        layers_dim={'h':[8, 32, 128], \n",
    "                                    'g':[128, 64, 32, 16, 8]})#.float()\n",
    "    else:\n",
    "        model = CNP_Net(io_dims=data_generator.io_dims)#.float()\n",
    "\n",
    "    if args.gpu:\n",
    "        model.to(device)\n",
    "        print('max gpu memory', torch.cuda.max_memory_allocated(torch.cuda.current_device()))\n",
    "        print('current gpu memory usage', torch.cuda.memory_allocated(torch.cuda.current_device()))\n",
    "#    for m, p in model.named_parameters():\n",
    "#        print(m, p)\n",
    "#    print(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    if args.fig_show:\n",
    "        fig = plt.figure()\n",
    "        fig.show()\n",
    "        ax = data_generator.make_fig_ax(fig)\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    for t in range(args.max_epoch):\n",
    "        # print(t)\n",
    "        loss = 0\n",
    "        #x, y = data_generator.generate_batch()\n",
    "        for p in range(args.batch_size):\n",
    "            # data\n",
    "            # x, y = data_generator.generate_sample()\n",
    "            train, test = data_generator.get_train_test_sample()\n",
    "            # print('train shape', train)\n",
    "            # print('test shape', test)\n",
    "            x_train, y_train = train\n",
    "            x_test, y_test = test\n",
    "\n",
    "            # print(x_train, y_train, x_test, y_test)\n",
    "            # print('shapes', x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "            # print(x_test.shape, y_test.shape, x_train.shape, y_train.shape)\n",
    "            \n",
    "#            if self.num_samples == 0:\n",
    "#                N = np.random.randint(self.num_samples_min, self.num_samples_max)\n",
    "#            else:\n",
    "#                N = self.num_samples\n",
    "#            if args.datasource == 'gp1d':\n",
    "#                gp = data_generator.gp\n",
    "#                gp.fit(x_train, y_train)\n",
    "#                #y_mu, y_cov = gp.predict(x_plot, return_cov=True)\n",
    "#                y_mu, y_cov = gp.predict(space_samples, return_cov=True)\n",
    "#            elif args.datasource == 'branin':\n",
    "#                pass\n",
    "\n",
    "            # print('train', x_train.shape, y_train.shape)\n",
    "    \n",
    "            training_set = torch.cat((torch.tensor(x_train),\n",
    "                        torch.tensor(y_train)),\n",
    "                    dim=1).float()\n",
    "            test_set = torch.cat((torch.tensor(x_test),\n",
    "                        torch.tensor(y_test)),\n",
    "                    dim=1).float()\n",
    "        \n",
    "            if args.gpu:\n",
    "                training_set = training_set.to(device)\n",
    "                test_set = test_set.to(device)\n",
    "                print('before forward current gpu memory usage', torch.cuda.memory_allocated(torch.cuda.current_device()))\n",
    "            print(training_set.shape, test_set.shape)\n",
    "            # print('train, test', training_set.shape, test_set.shape)\n",
    "            phi, log_prob = model(training_set, test_set)\n",
    "            # print('phi', phi.shape)\n",
    "            if args.gpu:\n",
    "                print('after forward current gpu memory usage', torch.cuda.memory_allocated(torch.cuda.current_device()))\n",
    "    \n",
    "            loss += -torch.sum(log_prob)\n",
    "        loss = loss / args.batch_size\n",
    "\n",
    "        if args.log:\n",
    "            with open(\"logs/%s/log.txt\"%args.log_folder, \"a\") as log_file:\n",
    "                log_file.write(\"%5d\\t%10.4f\\n\"%(t, loss.item()))\n",
    "        \n",
    "        if (t+1) % args.interval == 0:\n",
    "            print('%5d'%t, '%10.4f'%loss.item())\n",
    "            if args.fig_show:\n",
    "                plt.clf()\n",
    "                ax = data_generator.make_fig_ax(fig)\n",
    "            \n",
    "            # train, test points\n",
    "            # print(x_test.shape, y_test.shape)\n",
    "            # data_generator.plot_data(fig, np.concatenate((x_test, y_test), axis=1))\n",
    "            if args.fig_show:\n",
    "                data_generator.scatter_data(ax, np.concatenate((x_test, y_test), axis=1), c='y')\n",
    "                data_generator.scatter_data(ax, np.concatenate((x_train, y_train), axis=1), c='r')\n",
    "\n",
    "    #                if x_test.shape[1] == 1:\n",
    "    #                    plt.scatter(x_test, y_test, c='y')\n",
    "    #                    plt.scatter(x_train, y_train, c='r')\n",
    "    #            \n",
    "    #                    # plot gp prediction (base line)\n",
    "    #                    plot_fig(fig, x_plot, y_mu, y_cov)\n",
    "       \n",
    "                # plot model prediction\n",
    "    #            print(x_plot.shape)\n",
    "    #            test_set = torch.cat((torch.tensor(x_plot),\n",
    "    #                    torch.tensor(np.zeros(len(x_plot)).reshape(-1, 1))),\n",
    "    #                dim=1).float()\n",
    "                # print(space_samples)\n",
    "                test_set = torch.cat((torch.tensor(space_samples),\n",
    "                        torch.tensor(np.zeros(len(space_samples)).reshape(-1, 1))),\n",
    "                    dim=1).float()\n",
    "                if args.gpu:\n",
    "                    test_set = test_set.to(device)\n",
    "                # print('train, test', training_set.shape, test_set.shape)\n",
    "                phi, _ = model(training_set, test_set)\n",
    "                if args.gpu:\n",
    "                    phi = phi.cpu()\n",
    "                # print('phi', phi.shape)\n",
    "                predict_y_mu = phi[:,:data_generator.io_dims[1]].data.numpy()\n",
    "                predict_y_cov = phi[:,data_generator.io_dims[1]:].data.numpy()**2\n",
    "    #            predict_y_mu_, predict_y_cov_, _ = model(training_set, test_set)\n",
    "    #            predict_y_mu = predict_y_mu_.data.numpy()\n",
    "    #            predict_y_cov = np.diag(predict_y_cov_.data.numpy())**2\n",
    "    \n",
    "                # plot_fig(fig, x_plot, predict_y_mu, predict_y_cov, color='b')\n",
    "                # print(space_samples.shape, predict_y_mu.shape, predict_y_cov.shape)\n",
    "                data_generator.plot_data(ax, np.concatenate((space_samples, predict_y_mu, predict_y_cov), axis=1))\n",
    "                fig.canvas.draw()\n",
    "\n",
    "            if args.log:\n",
    "                plt.savefig('logs/%s/%05d.png'%(args.log_folder, t))\n",
    "                torch.save(model, \"logs/%s/%05d.pt\"%(args.log_folder, t))\n",
    "        print('before backward') \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('before backward') \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_1.8.0_cuda9.2",
   "language": "python",
   "name": "tf_1.8.0_cuda9.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
