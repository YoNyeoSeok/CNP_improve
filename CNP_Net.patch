--- /home/mlg/yys/CNP_improve/model.py
+++ /home/mlg/yys/CNP_improve/model.py
@@ -8,30 +8,22 @@
         self.net_g = Net_g(layers_dim['g'], io_dims)
         self.operator = torch.mean
         self.softplus = nn.Softplus()
-        self.b1 = nn.BatchNorm1d(9)
-        self.b2 = nn.BatchNorm1d(9)
 
     def forward(self, O, T):
+        import time
+        t = time.time()
         self.r = self.operator(self.net_h(O), dim=0).expand(T.shape[0], -1)
         self.xr = torch.cat((self.r, T[:,:self.io_dims[0]]), dim=1)
         self.phi = self.net_g(self.xr)
 
-        if self.io_dims[1] != 1: 
-            print("multivariate regression is not supported")
-            return
+#        if self.io_dims[1] != 1: 
+#            print("multivariate regression is not supported")
+#            return
         self.mu = self.phi[:,:self.io_dims[1]]
         self.sig = self.softplus(self.phi[:,self.io_dims[1]:])
-        self.cov = self.sig**2
         
-        log_probs = -0.5*torch.log(2*np.pi*self.cov) - 0.5*(self.mu-T[:,self.io_dims[0]:])**2/self.cov
-        log_prob = torch.sum(log_probs)
-        
-        #print(torch.cat((self.mu, self.cov), dim=1))
-        return torch.cat((self.mu, self.cov), dim=1), log_prob/len(log_probs)
-    #return self.phi, log_prob/len(log_probs)
+        log_probs = -0.5*torch.log(2*np.pi*self.sig**2) - 0.5*(self.mu-T[:,self.io_dims[0]:])**2/self.sig**2
 
-
-        """
 #        normals = [MultivariateNormal(mu, torch.diag(cov)) for mu, cov in 
 #                zip(self.mu, self.sig**2)]
 #        log_probs_ = [normals[i].log_prob(t[self.io_dims[0]:]) for i, t in enumerate(T)]
@@ -40,6 +32,7 @@
 #        print(l.shape, l_.shape)
 #        print([log_probs_)
 #        print(l.reshape(-1)-l_)
+        """
         log_probs = []
         def func(m, s, t):
             normal = MultivariateNormal(m, torch.diag(s**2))
@@ -80,4 +73,13 @@
 #        log_probs = [normals[i].log_prob(t[self.io_dims[0]:]) for i, t in enumerate(T)]
 #        print('log_probs', time.time() - t)    
         """
+        log_prob = torch.sum(log_probs)
+#        diff = 0
+#        for p, p_ in zip(l, l_):
+#            diff += p-p_
+#        print(diff.shape)
+#        print(diff)
+#        print(len(log_probs))
+        
+        return self.phi, log_prob/len(log_probs)
 